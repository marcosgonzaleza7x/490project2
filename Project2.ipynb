{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8903b2b-8874-4dd6-b177-7953d94da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddba916-8382-4851-95e3-2fe2413539c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cude' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec766f0d-60be-4558-9a08-eb7ff6f11d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D miniCNN\n",
    "class MiniCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_classes):\n",
    "        super(MiniCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 200, 128)  # Adjust input size based on your sequence length\n",
    "        self.fc2 = nn.Linear(128, output_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten the output for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c06532b-6037-450b-9241-99a6b787544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneDAlexNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(oneDAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(6)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# 1DAlexNet will run with other function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1baec9d5-6d1d-4d7f-bcf3-6d520ee2bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from alexnet example, nees to be formatted for oneDAlexnet\n",
    "def train(model, device, train_loader, optimizer, loss_fn, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_ids, (img, classes) in enumerate(train_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            img, classes = img.to(device), classes.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = loss_fn(output, classes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch_ids + 1) % 2 == 0:\n",
    "                print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch+1, batch_ids * len(img), len(train_loader.dataset),\n",
    "                    100. * batch_ids / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16342bc-7168-4506-a591-9c98d53ee28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from alexnet example needs to be formatted for oneDAlexnet\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for img, classes in test_loader:\n",
    "            img, classes = img.to(device), classes.to(device)\n",
    "            output = model(img)\n",
    "            test_loss += loss_fn(output, classes, reduction='sum').item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == classes).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c68fb8-c8dd-44b1-9160-7e147329f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing with only 1000 lines, \n",
    "#Read the first 1000 lines from merged_data.txt\n",
    "with open('merged_data.txt', 'r') as file:\n",
    "    lines = [next(file).strip() for _ in range(1000)]\n",
    "\n",
    "# Write the lines to a new file named 1000.txt\n",
    "with open('1000.txt', 'w') as outfile:\n",
    "    for line in lines:\n",
    "        # Split the line into label and sequence\n",
    "        label, sequence = line.split(' ', 1)\n",
    "        # Write the label and sequence on the same line, separated by a space\n",
    "        outfile.write(f\"{label} {sequence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cd4f89-08c2-4cca-8a33-fc30be8d1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalrows: 641810\n",
      "training rows: 481357\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_data(input_file, train_file, test_file, val_file, train_percent=0.75, test_percent=0.20):\n",
    "    # Read the input data file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # Calculate the number of rows for each split\n",
    "    total_rows = len(data)\n",
    "    print(\"totalrows:\", total_rows)\n",
    "    train_rows = int(train_percent * total_rows)\n",
    "    print(\"training rows:\", train_rows)\n",
    "    test_rows = int(test_percent * total_rows)\n",
    "    val_rows = total_rows - train_rows - test_rows\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Write data to train, test, and validation files\n",
    "    with open(train_file, 'w') as f:\n",
    "        f.writelines(data[:train_rows])\n",
    "\n",
    "    with open(test_file, 'w') as f:\n",
    "        f.writelines(data[train_rows:train_rows + test_rows])\n",
    "\n",
    "    with open(val_file, 'w') as f:\n",
    "        f.writelines(data[train_rows + test_rows:])\n",
    "\n",
    "# File paths\n",
    "input_file = \"merged_data.txt\" #testing performed with 1000.txt\n",
    "train_file = \"train_data.txt\"\n",
    "test_file = \"test_data.txt\"\n",
    "val_file = \"val_data.txt\"\n",
    "\n",
    "# Split data into train, test, and validation files\n",
    "split_data(input_file, train_file, test_file, val_file, train_percent=0.75, test_percent=0.20) #values can be changed to 80 train, and 20 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd46f0f-96f6-4696-a4f4-822abfd85373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-encoder function\n",
    "def one_hot_encoder(sequence):\n",
    "    sequence = sequence.upper()  # Convert sequence to uppercase so they all match\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    one_hot = np.zeros((len(sequence), 4), dtype=int)\n",
    "    for i, nucleotide in enumerate(sequence):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[i, mapping[nucleotide]] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22a867f-01b7-426d-ba15-05bd2fd77e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read sequences from a data file and apply one-hot encoding\n",
    "def process_data_file(input_file, output_file):\n",
    "    data = []\n",
    "    min_length = float('inf')  # Initialize with a large value\n",
    "    \n",
    "    # Read the input data file\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) >= 2:\n",
    "                label, sequence = parts[0], ' '.join(parts[1:])\n",
    "                sequence = sequence.upper()  # Convert sequence to uppercase\n",
    "                min_length = min(min_length, len(sequence))  # Update min_length\n",
    "                data.append((int(label), sequence))\n",
    "    \n",
    "    # Crop sequences to the minimum length\n",
    "    cropped_data = [(label, sequence[:min_length]) for label, sequence in data]\n",
    "    \n",
    "    # Write cropped data to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        for label, sequence in cropped_data:\n",
    "            file.write(f\"{label} {sequence}\\n\")\n",
    "    \n",
    "    return cropped_data  # Return the cropped data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f79a2bc-e1ca-4705-bc31-0a2edb984c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, sequence = self.data[idx]\n",
    "        # Convert sequence to one-hot encoded tensor\n",
    "        sequence_tensor = torch.tensor(one_hot_encoder(sequence), dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf59018-bea3-44df-96df-fb0456ccda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to tensor fromat\n",
    "def convert_to_tensor(dataset):\n",
    "    tensor_data = []\n",
    "    for sequence_tensor, label_tensor in dataset:\n",
    "        tensor_data.append((sequence_tensor, label_tensor))\n",
    "    return tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0cb4725-2854-4498-a305-393f29cc1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader after tensor converion\n",
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d3bf64-632c-4858-9cba-6f1360efa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data back to file\n",
    "def save_data_to_file(dataset, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sequence_tensor, label_tensor in dataset:\n",
    "            label = int(label_tensor.item())\n",
    "            sequence_str = ''.join([['A', 'C', 'G', 'T'][i] for i in sequence_tensor.argmax(dim=1)])\n",
    "            file.write(f\"{label} {sequence_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f133a27-d428-411b-8900-171acd621e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: torch.Size([512, 176, 4])\n",
      "Shape of label tensor: torch.Size([512])\n",
      "Label: tensor(1.)\n",
      "One-hot encoding:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "train_data = process_data_file('train_data.txt', \"train_data_after_encode.txt\")\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_dataset_tensor = convert_to_tensor(train_dataset)\n",
    "save_data_to_file(train_dataset_tensor, 'train_data_modified.txt')\n",
    "\n",
    "# Process testing data\n",
    "test_data = process_data_file('test_data.txt',\"test_data_after_encode.txt\")\n",
    "test_dataset = CustomDataset(test_data)\n",
    "test_dataset_tensor = convert_to_tensor(test_dataset)\n",
    "save_data_to_file(test_dataset_tensor, 'test_data_modified.txt')\n",
    "\n",
    "# Process validation data\n",
    "val_data = process_data_file('val_data.txt',\"val_data_after_encode.txt\")\n",
    "val_dataset = CustomDataset(val_data)\n",
    "val_dataset_tensor = convert_to_tensor(val_dataset)\n",
    "save_data_to_file(val_dataset_tensor, 'val_data_modified.txt')\n",
    "\n",
    "#start training the data loader\n",
    "train_dataloader = create_data_loader(train_dataset_tensor, batch_size = 512, shuffle=True) #chnaged train_ for testing \n",
    "\n",
    "# Iterate through the training data loader and print at least one label and one one-hot encoding\n",
    "for data, label in train_dataloader:\n",
    "    print(\"Shape of data tensor:\", data.shape)\n",
    "    print(\"Shape of label tensor:\", label.shape)\n",
    "    \n",
    "    print(\"Label:\", label[0])\n",
    "    print(\"One-hot encoding:\")\n",
    "    print(data[0])\n",
    "    break\n",
    "\n",
    "# Define the number of input channels (4 for one-hot encoding) and the number of output classes\n",
    "input_channels = 4\n",
    "output_classes = 2  # Assuming binary classification\n",
    "\n",
    "# Instantiate the mini CNN model\n",
    "mini_cnn_model = MiniCNN(input_channels, output_classes)\n",
    "\n",
    "#update for alexnet model\n",
    "input_channels = 1  # Assuming input sequences have one channel\n",
    "num_classes = 4 # Example number of classes\n",
    "model = oneDAlexNet(input_channels, num_classes)\n",
    "\n",
    "#the following needs to be formatted for 1DAlexnet\n",
    "#if __name__ == '__main__':\n",
    "    #seed = 42\n",
    "    #torch.manual_seed(seed)\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = oneDAlexNet(input_channels, num_classes = 4)  \n",
    "    #model.to(device)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #loss_fn = F.cross_entropy  # Cross-entropy loss is commonly used for classification tasks\n",
    "\n",
    "    #EPOCHS = 2\n",
    "    #for epoch in range(EPOCHS):\n",
    "       # train(model, device, train_loader, optimizer, loss_fn, epoch)\n",
    "       # test(model, device, test_loader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project1] *",
   "language": "python",
   "name": "conda-env-project1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
