{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8903b2b-8874-4dd6-b177-7953d94da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddba916-8382-4851-95e3-2fe2413539c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cude' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec766f0d-60be-4558-9a08-eb7ff6f11d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D miniCNN\n",
    "class MiniCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_classes):\n",
    "        super(MiniCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(64 * 42, 128) \n",
    "        self.fc2 = nn.Linear(128, output_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply full connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c06532b-6037-450b-9241-99a6b787544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current errors need addressing\n",
    "class OneDAlexNet(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(OneDAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(6)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Permute input tensor to match expected shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # If labels are provided, print the loss\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(x, labels)\n",
    "            print(\"Loss:\", loss.item())\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1baec9d5-6d1d-4d7f-bcf3-6d520ee2bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not called anymore\n",
    "def train(model,device,train_loader,optimizer,epochs):\n",
    "    print(\"inside train\")\n",
    "    model.train()\n",
    "    for batch_ids, (img, classes) in enumerate(train_loader):\n",
    "        classes=classes.type(torch.LongTensor)\n",
    "        img,classes=img.to(device),classes.to(device)\n",
    "        torch.autograd.set_detect_anomaly(True)     \n",
    "        optimizer.zero_grad()\n",
    "        output=model(img)\n",
    "        loss = loss_fn(output,classes)                \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(batch_ids +1) % 2 == 0:\n",
    "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch, batch_ids* len(img), len(train_loader.dataset),\n",
    "            100.*batch_ids / len(train_loader),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16342bc-7168-4506-a591-9c98d53ee28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not called anymore\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for img,classes in test_loader:\n",
    "            img,classes=img.to(device), classes.to(device)\n",
    "            y_hat=model(img)\n",
    "            test_loss+=F.nll_loss(y_hat,classes,reduction='sum').item()\n",
    "            _,y_pred=torch.max(y_hat,1)\n",
    "            correct+=(y_pred==classes).sum().item()\n",
    "        test_loss/=len(test_dataset)\n",
    "        print(\"\\n Test set: Avarage loss: {:.0f},Accuracy:{}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,correct,len(test_dataset),100.*correct/len(test_dataset)))\n",
    "        print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c68fb8-c8dd-44b1-9160-7e147329f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing with only 1000 lines, \n",
    "#Read the first 1000 lines from merged_data.txt\n",
    "with open('merged_data.txt', 'r') as file:\n",
    "    lines = [next(file).strip() for _ in range(1000)]\n",
    "\n",
    "# Write the lines to a new file named 1000.txt\n",
    "with open('1000.txt', 'w') as outfile:\n",
    "    for line in lines:\n",
    "        # Split the line into label and sequence\n",
    "        label, sequence = line.split(' ', 1)\n",
    "        # Write the label and sequence on the same line, separated by a space\n",
    "        outfile.write(f\"{label} {sequence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cd4f89-08c2-4cca-8a33-fc30be8d1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalrows: 641810\n",
      "training rows: 481357\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_data(input_file, train_file, test_file, val_file, train_percent=0.75, test_percent=0.20):\n",
    "    # Read the input data file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # Calculate the number of rows for each split\n",
    "    total_rows = len(data)\n",
    "    print(\"totalrows:\", total_rows)\n",
    "    train_rows = int(train_percent * total_rows)\n",
    "    print(\"training rows:\", train_rows)\n",
    "    test_rows = int(test_percent * total_rows)\n",
    "    val_rows = total_rows - train_rows - test_rows\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Write data \n",
    "    with open(train_file, 'w') as f:\n",
    "        f.writelines(data[:train_rows])\n",
    "\n",
    "    with open(test_file, 'w') as f:\n",
    "        f.writelines(data[train_rows:train_rows + test_rows])\n",
    "\n",
    "    with open(val_file, 'w') as f:\n",
    "        f.writelines(data[train_rows + test_rows:])\n",
    "\n",
    "# File paths\n",
    "input_file = \"merged_data.txt\" #testing performed with 1000.txt\n",
    "train_file = \"train_data.txt\"\n",
    "test_file = \"test_data.txt\"\n",
    "val_file = \"val_data.txt\"\n",
    "\n",
    "# Split data into train, test, and validation files\n",
    "split_data(input_file, train_file, test_file, val_file, train_percent=0.75, test_percent=0.20) #values can be changed to 80 train, and 20 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd46f0f-96f6-4696-a4f4-822abfd85373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-encoder function\n",
    "def one_hot_encoder(sequence):\n",
    "    sequence = sequence.upper()  # Convert sequence to uppercase so they all match\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    one_hot = np.zeros((len(sequence), 4), dtype=int)\n",
    "    for i, nucleotide in enumerate(sequence):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[i, mapping[nucleotide]] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22a867f-01b7-426d-ba15-05bd2fd77e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read sequences from a data file and apply one-hot encoding\n",
    "def process_data_file(input_file, output_file):\n",
    "    data = []\n",
    "    min_length = float('inf')  # Initialize with a large value\n",
    "    \n",
    "    # Read the input data file\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) >= 2:\n",
    "                label, sequence = parts[0], ' '.join(parts[1:])\n",
    "                sequence = sequence.upper()  # Convert sequence to uppercase\n",
    "                min_length = min(min_length, len(sequence))  \n",
    "                data.append((int(label), sequence))\n",
    "    \n",
    "    # Crop sequences to the minimum length\n",
    "    cropped_data = [(label, sequence[:min_length]) for label, sequence in data]\n",
    "    \n",
    "    # Write cropped data to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        for label, sequence in cropped_data:\n",
    "            file.write(f\"{label} {sequence}\\n\")\n",
    "    \n",
    "    return cropped_data  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f79a2bc-e1ca-4705-bc31-0a2edb984c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, sequence = self.data[idx]\n",
    "        # Convert sequence to one-hot encoded tensor\n",
    "        sequence_tensor = torch.tensor(one_hot_encoder(sequence), dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf59018-bea3-44df-96df-fb0456ccda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to tensor fromat\n",
    "def convert_to_tensor(dataset):\n",
    "    tensor_data = []\n",
    "    for sequence_tensor, label_tensor in dataset:\n",
    "        tensor_data.append((sequence_tensor, label_tensor))\n",
    "    return tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0cb4725-2854-4498-a305-393f29cc1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader after tensor converion\n",
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d3bf64-632c-4858-9cba-6f1360efa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data back to file\n",
    "def save_data_to_file(dataset, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sequence_tensor, label_tensor in dataset:\n",
    "            label = int(label_tensor.item())\n",
    "            sequence_str = ''.join([['A', 'C', 'G', 'T'][i] for i in sequence_tensor.argmax(dim=1)])\n",
    "            file.write(f\"{label} {sequence_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f133a27-d428-411b-8900-171acd621e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: torch.Size([512, 176, 4])\n",
      "New shape after format: torch.Size([512, 4, 176])\n",
      "Shape of label tensor: torch.Size([512])\n",
      "Label: tensor(1.)\n",
      "One-hot encoding:\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "Epoch 1, Loss: 546.1116464138031\n",
      "Epoch 2, Loss: 508.3407970368862\n",
      "Epoch 3, Loss: 489.7508575320244\n",
      "Epoch 4, Loss: 475.4971736073494\n",
      "Epoch 5, Loss: 461.9657298922539\n",
      "Epoch 6, Loss: 449.887325912714\n",
      "Epoch 7, Loss: 438.3457728624344\n",
      "Epoch 8, Loss: 426.9252362549305\n",
      "Epoch 9, Loss: 414.16783398389816\n",
      "Epoch 10, Loss: 400.6677056849003\n",
      "Epoch 11, Loss: 387.06328505277634\n",
      "Epoch 12, Loss: 373.8198654949665\n",
      "Epoch 13, Loss: 360.42165583372116\n",
      "Epoch 14, Loss: 346.68875566124916\n",
      "Epoch 15, Loss: 333.1572626531124\n"
     ]
    }
   ],
   "source": [
    "#main- process training data\n",
    "train_data = process_data_file('train_data.txt', \"train_data_after_encode.txt\")\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_dataset_tensor = convert_to_tensor(train_dataset)\n",
    "save_data_to_file(train_dataset_tensor, 'train_data_modified.txt')\n",
    "\n",
    "# Process testing data\n",
    "test_data = process_data_file('test_data.txt',\"test_data_after_encode.txt\")\n",
    "test_dataset = CustomDataset(test_data)\n",
    "test_dataset_tensor = convert_to_tensor(test_dataset)\n",
    "save_data_to_file(test_dataset_tensor, 'test_data_modified.txt')\n",
    "\n",
    "# Process validation data\n",
    "val_data = process_data_file('val_data.txt',\"val_data_after_encode.txt\")\n",
    "val_dataset = CustomDataset(val_data)\n",
    "val_dataset_tensor = convert_to_tensor(val_dataset)\n",
    "save_data_to_file(val_dataset_tensor, 'val_data_modified.txt')\n",
    "\n",
    "#start training the data loader\n",
    "train_loader = create_data_loader(train_dataset_tensor, batch_size = 512, shuffle=True) #changed train_dataset_tensor for testing, 512-> 64 \n",
    "\n",
    "# Iterate through the training data loader and print at least one label and one one-hot encoding\n",
    "for data, label in train_loader:\n",
    "    print(\"Shape of data tensor:\", data.shape)\n",
    "    data_permuted = data.permute(0,2,1)\n",
    "    print(\"New shape after format:\", data_permuted.shape)\n",
    "    print(\"Shape of label tensor:\", label.shape)\n",
    "    \n",
    "    print(\"Label:\", label[0])\n",
    "    print(\"One-hot encoding:\")\n",
    "    print(data[0])\n",
    "    break\n",
    "\n",
    "# input channels (4 for one-hot encoding) and the number of output classes\n",
    "input_channels = 4\n",
    "output_classes = 1  # binary\n",
    "\n",
    "mini_cnn_model = MiniCNN(input_channels, output_classes)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(mini_cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(15):  #epochs for testing 5->25\n",
    "    running_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data_permuted = data.permute(0, 2, 1)  #(batch_size, input_channels, sequence_length)\n",
    "        outputs = mini_cnn_model(data_permuted)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "    \n",
    "#update for alexnet model\n",
    "input_channels = 1  \n",
    "num_classes = 4 \n",
    "\n",
    "model = OneDAlexNet(input_channels, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Inside training loop\n",
    "#for data, labels in train_loader:\n",
    " #   optimizer.zero_grad()\n",
    " #   data_permuted = data.permute(0, 2, 1)\n",
    " #   output = model(data_permuted) \n",
    " #   loss = F.cross_entropy(output, labels)\n",
    " #   loss.backward()\n",
    " #   optimizer.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project1] *",
   "language": "python",
   "name": "conda-env-project1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
